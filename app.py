# âœ¨ Artify â€” buscador de convocatorias de arte para Fla â¤ï¸
# UI estilo â€œHÃ©rculesâ€ + motor AR curado + parsers dedicados
# SIN â€œdificultadâ€, deduplicaciÃ³n fuerte, y manejo de PDF/binarios.

import re, io, csv, time, socket
from datetime import date, timedelta
from urllib.parse import urlparse, urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
from bs4 import BeautifulSoup
import streamlit as st

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Config / Estilo
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Artify â€” buscador de convocatorias de arte para Fla â¤ï¸", layout="wide")
st.markdown("""
<style>
:root{--card-bg:#fff;--muted:#6b7280;}
.block-container{padding-top:2rem;padding-bottom:2rem;}
.topbar{display:flex;justify-content:space-between;align-items:center;margin-bottom:1rem}
.topbar h1{font-weight:800;margin:0}
.filterbox{border:1px solid #e5e7eb;border-radius:14px;padding:18px;margin-bottom:12px;background:#fafafa}
.badge{display:inline-block;background:#f3f4f6;color:#111827;border-radius:999px;padding:.2rem .6rem;font-size:.80rem;margin-right:.35rem}
.card{border:1px solid #e5e7eb;border-radius:16px;padding:18px;background:var(--card-bg);margin:10px 0}
.card h3{margin:0 0 6px 0}
.meta{color:var(--muted);font-size:.88rem;margin:.25rem 0}
.kpis{display:flex;gap:8px;flex-wrap:wrap;margin:.5rem 0}
.kpis .pill{background:#f9fafb;border:1px solid #e5e7eb;border-radius:10px;padding:.25rem .5rem;font-size:.85rem}
.btn-row{display:flex;gap:8px;flex-wrap:wrap;margin-top:.5rem}
hr.sep{border:none;border-top:1px solid #eee;margin:8px 0 16px}
.small{font-size:.9rem;color:var(--muted)}
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="topbar"><h1>âœ¨ Artify â€” buscador de convocatorias de arte para Fla â¤ï¸</h1></div>', unsafe_allow_html=True)
st.caption("Resultados en espaÃ±ol, con resumen automÃ¡tico. Motor curado para Argentina con parsers dedicados y manejo de PDFs.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Constantes / Seeds
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_WORKERS = 14
REQ_TIMEOUT = 9
HARD_TIME_LIMIT = 40  # mÃ¡s aire para encontrar varias
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

CURATED_AR_SEEDS = [
    "https://www.klemm.org.ar/",
    "https://premioklemm.klemm.org.ar/",
    "https://www.fnartes.gob.ar/",
    "https://palaisdeglace.cultura.gob.ar/",
    "https://www.cultura.gob.ar/",
    "https://enteculturaltucuman.gob.ar/",
    "https://www.fundacionosde.com.ar/",
    "https://museorosagalisteo.gob.ar/",
    "https://www.castagninomacro.org/",
    "https://museosiivori.buenosaires.gob.ar/",
    "https://www.catalogosparaartistas.com/convocatorias",
    "https://es.artealdia.com/Convocatorias",
    "https://www.recursosculturales.com/",
]

SKIP_HOSTS = (
    "instagram.com","facebook.com","x.com","twitter.com","tiktok.com",
    "youtube.com","linkedin.com","pinterest.","flickr.","tumblr.","vimeo.com"
)
SKIP_EXTS = (".jpg",".jpeg",".png",".gif",".webp",".doc",".docx",".xls",".xlsx",".zip",".rar")

GENERIC_BAD = re.compile(
    r"(pol[iÃ­]tica de privacidad|privacy|cookies|prensa|newsroom|contacto|"
    r"acerca|historia|colecci[oÃ³]n|press|terms?|condiciones|mapa del sitio)",
    re.I
)

KEYWORDS = ["convocatoria","premio","salÃ³n","salon","residenc","beca","open call","inscripciÃ³n","cierre","bases"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utilidades de red / binarios / links
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_url(u: str, base: str):
    if not u: return None
    u=u.strip()
    if u.startswith("//"): u="https:"+u
    if u.startswith("/"):  u=urljoin(base,u)
    if not re.match(r"^https?://", u): return None
    return u.split("#")[0]

def head_ok(u: str, timeout=7):
    if not u: return False
    try:
        r=requests.head(u, headers=HEADERS, timeout=timeout, allow_redirects=True)
        if r.status_code in (405,403):
            r=requests.get(u, headers=HEADERS, timeout=timeout, allow_redirects=True, stream=True)
        return 200<=r.status_code<400
    except (requests.RequestException, socket.error):
        return False

def fetch_bytes(url:str):
    r=requests.get(url, headers=HEADERS, timeout=REQ_TIMEOUT, allow_redirects=True, stream=True)
    r.raise_for_status()
    ct=r.headers.get("Content-Type","").lower()
    data=r.content
    return data, ct, r.url

def is_pdf(data:bytes, ct:str):
    if "application/pdf" in ct: return True
    return data[:4] == b"%PDF"

def bytes_to_html(data:bytes, ct:str):
    if is_pdf(data, ct): return "", None
    try: return data.decode("utf-8"), "utf-8"
    except UnicodeDecodeError:
        try: return data.decode("latin-1"), "latin-1"
        except UnicodeDecodeError: return "", None

def cleanup_text(s:str):
    if not s: return ""
    s = s.replace("\x00"," ")
    s = re.sub(r"[\uFFFD]{2,}", " ", s)
    s = re.sub(r"[^\x09\x0A\x0D\x20-\x7E\u00A0-\uFFFF]", " ", s)
    s = re.sub(r"\s{2,}", " ", s).strip()
    return s

def looks_gibberish(s:str):
    if not s: return False
    if s.count("ï¿½") >= 5: return True
    junk = len(re.findall(r"[^0-9A-Za-zÃÃ‰ÃÃ“ÃšÃ¡Ã©Ã­Ã³ÃºÃ‘Ã±ÃœÃ¼Â¿?Â¡!;:.,()\\[\\]{}/\\-â€“â€”%â‚¬$\\s]", s))
    return junk > 0.35*max(1,len(s))

def link_button(label: str, url: str):
    if not url: return
    try: st.link_button(label, url)
    except Exception: st.markdown(f"[{label}]({url})")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Fechas ES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MONTHS={'enero':1,'febrero':2,'marzo':3,'abril':4,'mayo':5,'junio':6,'julio':7,'agosto':8,'septiembre':9,'setiembre':9,'octubre':10,'noviembre':11,'diciembre':12}
def _mk_date(y,m,d):
    try: return date(y,m,d)
    except: return None
def parse_spanish_date(txt:str):
    if not txt: return None
    s=str(txt).lower().replace("Âº","").replace("Â°","")
    s=re.sub(r"(\d)(?:st|nd|rd|th)", r"\1", s)
    m=re.search(r"(\d{1,2})\s+de\s+([a-zÃ¡]+)\s+de\s+(\d{4})", s)
    if m:
        d=int(m.group(1)); mon=m.group(2).replace("Ã¡","a"); y=int(m.group(3))
        if mon in MONTHS: return _mk_date(y, MONTHS[mon], d)
    m=re.search(r"(\d{1,2})[\/\-](\d{1,2})[\/\-](\d{2,4})", s)
    if m:
        a=int(m.group(1)); b=int(m.group(2)); y=int(m.group(3))
        if y<100: y+=2000
        return _mk_date(y,b,a) or _mk_date(y,a,b)
    m=re.search(r"(\d{1,2})\.(\d{1,2})\.(\d{4})", s)
    if m: return _mk_date(int(m.group(3)), int(m.group(2)), int(m.group(1)))
    return None
DATE_PATS=[
    r"(?:fecha(?:\s+l[iÃ­]mite)?(?:\s+de)?\s*(?:aplicaci[oÃ³]n|postulaci[oÃ³]n|cierre|presentaci[oÃ³]n)?:?\s*)(\d{1,2}\s+de\s+\w+\s+\d{4})",
    r"(?:cierran?\s+el\s+|cierra\s+el\s+|hasta el\s+)(\d{1,2}\s+de\s+\w+\s+\d{4})",
    r"(\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4})",
]
RANGE_PATS=[
    r"del\s+(\d{1,2}\s+de\s+\w+\s+de\s+\d{4})\s+al\s+(\d{1,2}\s+de\s+\w+\s+de\s+\d{4})",
    r"del\s+(\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4})\s+al\s+(\d{1,2}[\/\-]\d{1,2}[\/\-]\d{2,4})",
]
def extract_deadline(text:str):
    if not text: return None
    s=str(text)
    for pat in DATE_PATS:
        m=re.search(pat,s,re.I)
        if m:
            d=parse_spanish_date(m.group(1))
            if d: return d
    m=re.search(r"\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{2,4}", s)
    if m:
        d=parse_spanish_date(m.group(0))
        if d: return d
    return None
def extract_range(text:str):
    if not text: return (None,None)
    s=str(text)
    for pat in RANGE_PATS:
        m=re.search(pat,s,re.I)
        if m: return parse_spanish_date(m.group(1)), parse_spanish_date(m.group(2))
    m=re.search(r"hasta(?:\s+el)?\s+([^\.;,\n]+)", s, re.I)
    if m: return None, parse_spanish_date(m.group(1))
    return (None, extract_deadline(s))
def days_left(d): return None if not d else (d - date.today()).days

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# IA liviana: tÃ­tulo + resumen
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def smart_title_guess(title:str, text:str, domain:str):
    raw = (title or "").strip()
    if raw and not re.fullmatch(r"(convocatoria|home|inicio|noticias?)", raw, re.I):
        base = raw
    else:
        base = ""
    parts = re.split(r"\s*[|\-â€”â€“Â·]\s*", base) if base else []
    if not parts:
        m=re.search(r"(premio|sal[oÃ³]n|residenc\w+|beca|open call|convocatoria)[^\.]{0,80}", (text or ""), re.I)
        if m: parts=[m.group(0)]
    if not parts:
        return f"Convocatoria ({domain.replace('www.','')})"
    best=None; score_best=-1
    for p in parts:
        s=0
        s+=sum(k in p.lower() for k in ["premio","salÃ³n","salon","residenc","beca","convocatoria","open call"])
        s+=len(re.findall(r"20\d{2}", p))
        if s>score_best: score_best=s; best=p
    def friendly_tc(x):
        words=x.split()
        out=[]
        for w in words:
            if len(w)<=3 and w.isupper(): out.append(w)
            elif w.lower() in {"de","del","la","las","los","y","en","a","al","para"}: out.append(w.lower())
            else: out.append(w[:1].upper()+w[1:].lower())
        return " ".join(out)
    return friendly_tc(best.strip(" .:;-"))[:140]

def sentences(text:str): return [s.strip() for s in re.split(r"(?<=[\.\!\?])\s+", text) if s.strip()]
def resumen_ia(text:str,n=3,max_chars=360):
    txt=cleanup_text(text or "")
    if not txt or looks_gibberish(txt): return "Documento o publicaciÃ³n sin descripciÃ³n legible. AbrÃ­ â€œBases / Reglamentoâ€ para ver requisitos."
    sents=sentences(txt)
    scored=[]
    for s in sents:
        score=sum(int(k in s.lower()) for k in KEYWORDS)+len(re.findall(r"\d{4}", s))
        scored.append((score,s))
    scored.sort(reverse=True)
    chosen=[s for _,s in scored[:n]] or sents[:n]
    res=" ".join(chosen)
    return (res[:max_chars]+"â€¦") if len(res)>max_chars else res

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Parse genÃ©rico (maneja PDF/binario) + relevancia
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_relevant(text:str, title:str):
    blob=(title or "")+" "+(text or "")
    if GENERIC_BAD.search(blob): return False
    # al menos 1 palabra clave
    return any(k in blob.lower() for k in KEYWORDS)

def extract_title_desc(soup:BeautifulSoup):
    title=""
    for sel in ["meta[property='og:title']","meta[name='twitter:title']"]:
        m=soup.select_one(sel)
        if m and m.get("content"): title=m["content"]; break
    if not title:
        h1=soup.select_one("h1")
        if h1: title=cleanup_text(h1.get_text(" "))
    if not title:
        t=soup.select_one("title")
        if t: title=cleanup_text(t.get_text())
    desc=""
    for sel in ["meta[name='description']","meta[property='og:description']"]:
        m=soup.select_one(sel)
        if m and m.get("content"): desc=m["content"]; break
    if not desc:
        p=soup.select_one("p")
        if p: desc=cleanup_text(p.get_text(" "))
    return (title or "Convocatoria"), (desc or "")

def type_guess(text:str):
    s=(text or "").lower()
    if "residenc" in s: return "Residencias"
    if "beca" in s: return "Becas"
    if "premio" in s or "salÃ³n" in s or "salon" in s or "concurso" in s: return "Concursos"
    if "exposiciÃ³n" in s or "exposicion" in s: return "Exposiciones"
    return "Convocatorias"

def guess_location(text:str):
    s=(text or "").lower()
    if "argentina" in s or "buenos aires" in s or "caba" in s: return "Argentina"
    if "internacional" in s: return "Internacional"
    return "â€”"
def scope_from_location(loc:str):
    if loc=="Argentina": return "AR"
    if loc=="â€”": return "UNK"
    return "EX"

def extract_key_data(text:str):
    s=(text or "")
    m_amt=re.search(r"(USD|US\$|â‚¬|\$)\s?([\d\.\,]+)", s, re.I)
    premio=f"{m_amt.group(1).upper()} {m_amt.group(2)}" if m_amt else "â€”"
    m_slots=re.search(r"(\d+)\s+(cupos|ganadores|becas|finalistas)", s, re.I)
    cupos=m_slots.group(1) if m_slots else "â€”"
    m_fee=re.search(r"(?:fee|arancel|inscripci[oÃ³]n)\s*(?:de)?\s*(USD|US\$|â‚¬|\$)?\s*([\d\.\,]+)", s, re.I)
    fee=(m_fee.group(1) or "$")+" "+m_fee.group(2) if m_fee else "0"
    return premio, cupos, fee

def best_links(soup: BeautifulSoup, base_url: str):
    links={"principal":None,"bases":None,"inscripcion":None,"pdfs":[]}
    og=soup.select_one("meta[property='og:url']")
    if og and og.get("content"): links["principal"]=normalize_url(og["content"],base_url)
    if not links["principal"]:
        can=soup.select_one("link[rel='canonical']")
        if can and can.get("href"): links["principal"]=normalize_url(can["href"],base_url)
    candidates=[]
    for a in soup.select("a[href]"):
        href=normalize_url(a["href"], base_url)
        if not href: continue
        text=(a.get_text(" ") or "").strip().lower()
        candidates.append((text, href))
        if href.lower().endswith(".pdf"): links["pdfs"].append(href)
    def pick(patterns, avoid_pdf=True):
        for txt, href in candidates:
            if avoid_pdf and href.lower().endswith(".pdf"): continue
            if any(p in txt for p in patterns): return href
        return None
    links["inscripcion"]=pick(["inscrip","postul","apply","registro","formulario","aplicar"])
    links["bases"]=pick(["base","reglamento","condicion","tÃ©rmino","terms","rules"], avoid_pdf=False)
    if not links["principal"]:
        for txt, href in candidates:
            if href and not href.endswith("#"): links["principal"]=href; break
    for k in ["inscripcion","bases","principal"]:
        if links[k] and not head_ok(links[k], timeout=5): links[k]=None
    if not links["bases"]:
        for p in links["pdfs"]:
            if head_ok(p, timeout=5): links["bases"]=p; break
    return links

def parse_generic(url:str):
    try:
        data, ct, final = fetch_bytes(url)
        html, enc = bytes_to_html(data, ct)

        if not html:
            # Es PDF/binario â†’ tarjeta mÃ­nima (sin texto roto)
            domain=urlparse(final).netloc.replace("www.","")
            fname = final.split("/")[-1]
            title = smart_title_guess(fname, "", domain)
            return {
                "source": domain, "title": title or f"Documento PDF â€” {domain}", "url": final,
                "open_at": None, "deadline": None, "type": "Convocatorias",
                "location":"â€”","scope":"UNK", "prize":"â€”","slots":"â€”","fee":"0","free":True,
                "summary": "Documento PDF de bases / reglamento. AbrÃ­ el enlace para ver requisitos.",
                "links": {"principal": final, "bases": final, "inscripcion": None, "pdfs":[final]},
            }

        soup=BeautifulSoup(html, "html.parser")
        raw_title, meta_desc = extract_title_desc(soup)
        full = cleanup_text(soup.get_text(" "))
        if not is_relevant(full, raw_title):  # filtramos pÃ¡ginas genÃ©ricas
            return None

        abre, cierra = extract_range(full)
        tipo = type_guess(raw_title+" "+meta_desc+" "+full)
        loc  = guess_location(raw_title+" "+full)
        premio, cupos, fee = extract_key_data(full)
        titulo = smart_title_guess(raw_title, full, urlparse(final).netloc)
        resumen = resumen_ia(meta_desc if meta_desc else full)
        links = best_links(soup, final)
        free = bool(re.search(r"(sin costo|sin cargo|gratuit[oa]|gratis)", full.lower()))
        return {
            "source": urlparse(final).netloc.replace("www.",""),
            "title": titulo, "url": final,
            "open_at": abre, "deadline": cierra,
            "type": tipo, "location": loc, "scope": scope_from_location(loc),
            "prize": premio, "slots": cupos, "fee": fee, "free": free,
            "summary": resumen, "links": links
        }
    except Exception:
        return None

# Parsers dedicados (mejoran tÃ­tulo/links/fechas)
def parse_klemm(url:str):
    rec = parse_generic(url);  ############################################################################
    if not rec: return None
    try:
        data, ct, final = fetch_bytes(url)
        html, enc = bytes_to_html(data, ct)
        if html:
            soup=BeautifulSoup(html,"html.parser")
            full=cleanup_text(soup.get_text(" "))
            if "premio" in full.lower() and "klemm" in full.lower():
                rec["type"]="Concursos"
                if "klemm" not in rec["title"].lower():
                    rec["title"]=smart_title_guess("Premio Klemm", full, urlparse(final).netloc)
            for a in soup.select("a[href]"):
                href=normalize_url(a["href"], final) or ""
                tx=(a.get_text(" ") or "").lower()
                if "inscrip" in tx or "postul" in tx or "premioklemm" in href:
                    rec["links"]["inscripcion"]=href
                if "base" in tx or href.lower().endswith(".pdf"):
                    rec["links"]["bases"]=href
    except Exception: pass
    return rec

def parse_fna(url:str):
    rec = parse_generic(url)
    if not rec: return None
    try:
        data, ct, final = fetch_bytes(url)
        html, enc = bytes_to_html(data, ct)
        if html:
            soup=BeautifulSoup(html,"html.parser"); full=cleanup_text(soup.get_text(" "))
            if "beca" in full.lower(): rec["type"]="Becas"
            dl=extract_deadline(full);  rec["deadline"]=dl or rec["deadline"]
            for a in soup.select("a[href]"):
                href=normalize_url(a["href"], final) or ""; tx=(a.get_text(" ") or "").lower()
                if any(k in tx for k in ["inscrip","postul","formulario","aplicar"]) or "forms.gle" in href:
                    rec["links"]["inscripcion"]=href
                if "base" in tx or "reglamento" in tx:
                    rec["links"]["bases"]=href
            if "fondo nacional de las artes" not in rec["title"].lower():
                rec["title"]="FNA â€” " + rec["title"]
    except Exception: pass
    return rec

def parse_palais(url:str):
    rec = parse_generic(url)
    if not rec: return None
    try:
        data, ct, final = fetch_bytes(url)
        html, enc = bytes_to_html(data, ct)
        if html:
            soup=BeautifulSoup(html,"html.parser"); full=cleanup_text(soup.get_text(" "))
            if "salÃ³n nacional" in full.lower() or "salon nacional" in full.lower():
                rec["type"]="Concursos"
                if "salÃ³n nacional" not in rec["title"].lower():
                    rec["title"]="SalÃ³n Nacional â€” " + rec["title"]
            dl=extract_deadline(full); rec["deadline"]=dl or rec["deadline"]
            for a in soup.select("a[href]"):
                href=normalize_url(a["href"], final) or ""; tx=(a.get_text(" ") or "").lower()
                if "inscrip" in tx or "postul" in tx: rec["links"]["inscripcion"]=href
                if "base" in tx or "reglamento" in tx or href.lower().endswith(".pdf"): rec["links"]["bases"]=href
    except Exception: pass
    return rec

def parse_tucuman(url:str):
    rec = parse_generic(url)
    if not rec: return None
    try:
        data, ct, final = fetch_bytes(url)
        html, enc = bytes_to_html(data, ct)
        if html:
            soup=BeautifulSoup(html,"html.parser"); full=cleanup_text(soup.get_text(" "))
            if "salÃ³n nacional de tucumÃ¡n" in full.lower():
                rec["type"]="Concursos"
                if "tucumÃ¡n" not in rec["title"].lower():
                    rec["title"]="SalÃ³n Nacional de TucumÃ¡n â€” " + rec["title"]
            dl=extract_deadline(full); rec["deadline"]=dl or rec["deadline"]
            for a in soup.select("a[href]"):
                href=normalize_url(a["href"], final) or ""; tx=(a.get_text(" ") or "").lower()
                if "inscrip" in tx or "postul" in tx: rec["links"]["inscripcion"]=href
                if "base" in tx or "reglamento" in tx or href.lower().endswith(".pdf"): rec["links"]["bases"]=href
    except Exception: pass
    return rec

def parse_osde(url:str):
    rec = parse_generic(url)
    if not rec: return None
    try:
        data, ct, final = fetch_bytes(url)
        html, enc = bytes_to_html(data, ct)
        if html:
            soup=BeautifulSoup(html,"html.parser"); full=cleanup_text(soup.get_text(" "))
            if "premio" in full.lower() and "osde" in full.lower():
                rec["type"]="Concursos"
                if "osde" not in rec["title"].lower():
                    rec["title"]="Premio OSDE â€” " + rec["title"]
            dl=extract_deadline(full); rec["deadline"]=dl or rec["deadline"]
            for a in soup.select("a[href]"):
                href=normalize_url(a["href"], final) or ""; tx=(a.get_text(" ") or "").lower()
                if "inscrip" in tx or "postul" in tx: rec["links"]["inscripcion"]=href
                if "base" in tx or "reglamento" in tx or href.lower().endswith(".pdf"): rec["links"]["bases"]=href
    except Exception: pass
    return rec

DOMAIN_PARSERS = {
    "klemm.org.ar": parse_klemm,
    "premioklemm.klemm.org.ar": parse_klemm,
    "fnartes.gob.ar": parse_fna,
    "palaisdeglace.cultura.gob.ar": parse_palais,
    "cultura.gob.ar": parse_palais,
    "enteculturaltucuman.gob.ar": parse_tucuman,
    "fundacionosde.com.ar": parse_osde,
}

def parse_page(url:str):
    host=urlparse(url).netloc.lower().replace("www.","")
    parser=DOMAIN_PARSERS.get(host, parse_generic)
    return parser(url)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Crawler curado AR (BFS corto)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def seems_call(url:str, text:str):
    u=url.lower()
    if any(k in u for k in ("/convocatoria","/convocatorias","/premio","/residenc","/salon","/salÃ³n","/beca")):
        return True
    s=(text or "").lower()
    return any(k in s for k in ("convocatoria","premio","salÃ³n","salon","residenc","beca","open call","exposiciÃ³n","exposicion"))

@st.cache_data(ttl=7200, show_spinner=False)
def crawl_site_for_calls(seed:str, per_site_limit:int=16):
    host=urlparse(seed).netloc
    visited=set(); queue=[seed]; found=[]
    while queue and len(visited)<per_site_limit:
        url=queue.pop(0)
        if url in visited: continue
        visited.add(url)
        try:
            data, ct, final = fetch_bytes(url)
            html, enc = bytes_to_html(data, ct)
        except Exception:
            continue
        soup=BeautifulSoup(html or "<html></html>","html.parser")
        text=cleanup_text(soup.get_text(" ")) if html else ""

        if seems_call(url, text) and not GENERIC_BAD.search((soup.title.get_text() if soup.title else "")+" "+text):
            found.append(url)

        for a in soup.select("a[href]"):
            href=a["href"].strip()
            if not href or href.startswith("#") or href.startswith("mailto:"): continue
            u=urljoin(url, href)
            p=urlparse(u)
            if p.netloc!=host: continue
            if any(p.path.lower().endswith(ext) for ext in SKIP_EXTS): continue
            if any(b in p.netloc for b in SKIP_HOSTS): continue
            if u not in visited and u not in queue and len(queue) < per_site_limit*2:
                queue.append(u)
    uniq=[]; seen=set()
    for u in found:
        base=u.split("#")[0]
        if base in seen: continue
        seen.add(base); uniq.append(base)
    return uniq[:per_site_limit]

def gather_curated_ar(total_limit:int):
    per=max(6, total_limit // max(1,len(CURATED_AR_SEEDS)))
    urls=[]
    for s in CURATED_AR_SEEDS:
        urls += crawl_site_for_calls(s, per_site_limit=per)
    clean=[]; seen=set()
    for u in urls:
        host=urlparse(u).netloc.lower()
        if any(h in host for h in SKIP_HOSTS): continue
        base=u.split("#")[0]
        if base in seen: continue
        seen.add(base); clean.append(base)
        if len(clean)>=total_limit: break
    return clean

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FRONTEND (estilo â€œHÃ©rculesâ€) â€” SIN dificultad
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown('<div class="filterbox">', unsafe_allow_html=True)
c1, c2 = st.columns([0.7,0.3])
with c1: query = st.text_input("BÃºsqueda", placeholder="residencia, FNA, performance")
with c2: ordenar = st.selectbox("Ordenar por", ["Fecha lÃ­mite", "TÃ­tulo"])

c3, c4, c5 = st.columns([0.6,0.25,0.15])
with c3:
    categorias = st.multiselect("CategorÃ­as", ["Becas","Residencias","Concursos","Exposiciones","Convocatorias"],
                                default=["Becas","Residencias","Concursos","Convocatorias"])
with c4: ubicacion = st.selectbox("UbicaciÃ³n", ["Todas","Argentina","Internacional"])
with c5: sin_costo = st.checkbox("Solo sin arancel", value=False)

c6, c7, c8 = st.columns([0.25,0.25,0.5])
with c6:  desde = st.date_input("Desde", value=None)
with c7:  hasta = st.date_input("Hasta", value=None)
with c8:  pass

c9, c10 = st.columns([0.3,0.7])
with c9: do_search = st.button("Buscar", type="primary", use_container_width=True)
with c10:
    if st.button("Limpiar", use_container_width=True): st.experimental_rerun()
st.markdown('</div>', unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RUN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if do_search:
    t0=time.time()
    urls = gather_curated_ar(total_limit=96)

    results=[]; done=0; prog=st.progress(0)
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futs=[ex.submit(parse_page,u) for u in urls]
        for f in as_completed(futs):
            rec=None
            try: rec=f.result()
            except: rec=None
            if rec: results.append(rec)
            done+=1; prog.progress(min(1.0, done/max(1,len(futs))))
            if time.time()-t0 > HARD_TIME_LIMIT: break
    prog.empty()

    # DeduplicaciÃ³n fuerte: dominio + tÃ­tulo normalizado
    dedup={}
    def norm_title(t): return re.sub(r"\W+"," ", (t or "").strip().lower())
    for r in results:
        if not r: continue
        key=(r["source"], norm_title(r["title"]))
        # preferimos el que tiene deadline y link de inscripcion
        cur=dedup.get(key)
        def score(x):
            s=0
            if x.get("deadline"): s+=2
            if x.get("links",{}).get("inscripcion"): s+=2
            if x.get("links",{}).get("bases"): s+=1
            return s
        if cur is None or score(r)>score(cur): dedup[key]=r
    results=list(dedup.values())

    map_cats = {"Becas":"Becas","Residencias":"Residencias","Concursos":"Concursos","Exposiciones":"Exposiciones","Convocatorias":"Convocatorias"}

    def keep(r):
        if sin_costo and not r.get("free"): return False
        if ubicacion=="Argentina" and r.get("scope")!="AR": return False
        if ubicacion=="Internacional" and r.get("scope")=="AR": return False
        if categorias and r.get("type") not in [map_cats[c] for c in categorias]: return False
        d=r.get("deadline")
        if desde and d and d < desde: return False
        if hasta and d and d > hasta: return False
        if query:
            blob=(r.get("title","")+" "+r.get("summary","")).lower()
            if query.lower() not in blob: return False
        return True

    items=[x for x in results if keep(x)]

    if ordenar=="Fecha lÃ­mite":
        items.sort(key=lambda r:(r.get("deadline") is None, r.get("deadline") or date(2100,1,1)))
    else:
        items.sort(key=lambda r:r.get("title",""))

    left, right = st.columns([0.5,0.5])
    with left: st.caption(f"**{len(items)} resultados**  Â·  â± {round(time.time()-t0,1)} s")
    with right:
        if items:
            buf=io.StringIO(); w=csv.writer(buf)
            w.writerow(["titulo","url","fuente","categoria","ubicacion","ambito","abre","cierra","premio","cupos","fee","resumen"])
            for c in items:
                w.writerow([
                    c["title"], c["url"], c["source"], c["type"], c["location"], c["scope"],
                    c["open_at"].strftime("%Y-%m-%d") if c.get("open_at") else "",
                    c["deadline"].strftime("%Y-%m-%d") if c.get("deadline") else "",
                    c["prize"], c["slots"], c["fee"], c["summary"]
                ])
            st.download_button("ğŸ“„ Exportar CSV", buf.getvalue(), "artify_convocatorias.csv", "text/csv")

            def make_ics(items):
                def dtfmt(d): return d.strftime("%Y%m%d")
                ics=["BEGIN:VCALENDAR","VERSION:2.0","PRODID:-//Artify//Convocatorias//ES"]
                for c in items:
                    if not c.get("deadline"): continue
                    desc=(c.get("summary","")[:200]).replace("\n"," ")
                    ics+=["BEGIN:VEVENT",
                          f"SUMMARY:{c['title']} (cierre)",
                          f"DTSTART;VALUE=DATE:{dtfmt(c['deadline'])}",
                          f"DTEND;VALUE=DATE:{dtfmt(c['deadline']+timedelta(days=1))}",
                          f"DESCRIPTION:{desc}  URL: {c.get('url','')}",
                          "END:VEVENT"]
                ics.append("END:VCALENDAR")
                return "\n".join(ics)
            st.download_button("ğŸ“… Exportar ICS", make_ics(items), "artify_convocatorias.ics", "text/calendar")

    st.markdown("<hr class='sep'/>", unsafe_allow_html=True)

    if not items:
        st.info("No hubo resultados con estos filtros. ProbÃ¡ quitar 'Solo sin arancel' o ampliar fechas.")

    for r in items:
        open_txt = r["open_at"].strftime("%d/%m/%Y") if r.get("open_at") else "â€”"
        dl=r.get("deadline")
        dl_txt = dl.strftime("%d/%m/%Y") if dl else "Sin dato"
        left = days_left(dl)
        urgency = "ğŸŸ¢" if left is None else ("ğŸŸ¡" if left and left<=21 else "ğŸŸ¢")
        if left is not None and left <= 7: urgency="ğŸ”´"

        st.markdown('<div class="card">', unsafe_allow_html=True)
        hA, hB = st.columns([0.72,0.28])
        with hA:
            st.markdown(f"### {r['title']}")
            chips = " ".join([
                f"<span class='badge'>{r['type']}</span>",
                f"<span class='badge'>{r['location']}</span>",
                f"<span class='badge'>{r['source']}</span>",
                f"<span class='badge'>{'Sin costo' if r.get('free') else 'Con arancel'}</span>",
            ])
            st.markdown(chips, unsafe_allow_html=True)
            st.markdown(f"<div class='meta'><b>Abre:</b> {open_txt} &nbsp;â€¢&nbsp; <b>Cierra:</b> {dl_txt} {f'({left} dÃ­as)' if left is not None else ''} &nbsp;{urgency}</div>", unsafe_allow_html=True)
            st.write(r["summary"])
        with hB:
            st.markdown("<div class='kpis'>", unsafe_allow_html=True)
            st.markdown(f"<span class='pill'>Premio: {r['prize']}</span>", unsafe_allow_html=True)
            st.markdown(f"<span class='pill'>Cupos: {r['slots']}</span>", unsafe_allow_html=True)
            st.markdown(f"<span class='pill'>Fee: {r['fee']}</span>", unsafe_allow_html=True)
            st.markdown("</div>", unsafe_allow_html=True)
            st.markdown("<div class='btn-row'>", unsafe_allow_html=True)
            links=r.get("links",{})
            if links.get("inscripcion"): link_button("ğŸ“ Postular / InscripciÃ³n", links["inscripcion"])
            if links.get("bases"):       link_button("ğŸ“„ Bases / Reglamento",   links["bases"])
            link_button("ğŸŒ Abrir publicaciÃ³n", links.get("principal") or r["url"])
            st.markdown("</div>", unsafe_allow_html=True)
        st.markdown("</div>", unsafe_allow_html=True)

else:
    st.info("ElegÃ­ filtros y dale a **Buscar**. QuitÃ© la â€œdificultadâ€ y reforcÃ© la deduplicaciÃ³n para que no repita Klemm ni genÃ©ricos.")
